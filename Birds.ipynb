{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Birds.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# General imports"
      ],
      "metadata": {
        "id": "7KD8-l9niok2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_iVa6BHhh09C"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Delete all folders in data"
      ],
      "metadata": {
        "id": "ndd49vSSmu1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for root, dirs, files in os.walk(\"Data\", topdown=True):\n",
        "    for name in files:\n",
        "        os.remove(os.path.join(root, name))\n",
        "    for name in dirs:\n",
        "        os.rmdir(os.path.join(root, name))\n",
        "\n",
        "os.rmdir(\"Data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "cpx84Qoxm3dg",
        "outputId": "ae5f6cac-336d-47cb-ebe9-9f06999fa1ca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-352a67c7864c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install special libs"
      ],
      "metadata": {
        "id": "VUkugR39jtvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nw3c3sljxSQ",
        "outputId": "4eaf8933-5b31-49fe-e396-04fb4fe7af5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataEngine"
      ],
      "metadata": {
        "id": "9s5y9oJrjTEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import json\n",
        "import glob\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "\n",
        "def downloas_bird_sounds(query):\n",
        "    \"\"\"\n",
        "        Downloads the bird sounds from www.xeno-canto.org\n",
        "        param:\n",
        "          query: Name of the bird. Blank space with %20\n",
        "    \"\"\"\n",
        "    # GET ALL FILENAMES AND PATHS OF THE QUERY BIRD\n",
        "    url = 'https://www.xeno-canto.org/api/2/recordings?query=' + query\n",
        "\n",
        "    dataFolder = \"Data/\" + query.replace(\"%20\", \"_\") + \"/\"\n",
        "    mp3Folder = dataFolder + \"mp3/\"\n",
        "    arrayFolder = dataFolder + \"arrays/\"\n",
        "    predTestFolder = dataFolder + \"preTest/\"\n",
        "\n",
        "    os.mkdir(\"Data\")\n",
        "    os.mkdir(dataFolder)\n",
        "    os.mkdir(mp3Folder)\n",
        "    os.mkdir(arrayFolder)\n",
        "    os.mkdir(predTestFolder)\n",
        "\n",
        "    filename = wget.download(url, dataFolder + 'recordings.json')\n",
        "    print(filename)\n",
        "\n",
        "    # Get the json entries from your downloaded json\n",
        "    jsonFile = open(dataFolder + 'recordings.json', 'r')\n",
        "    values = json.load(jsonFile)\n",
        "    jsonFile.close()\n",
        "\n",
        "    # Create a pandas dataframe of records & convert to .csv file\n",
        "    record_df = pd.DataFrame(values['recordings'])\n",
        "    record_df.to_csv(dataFolder + 'xc-noca.csv', index=False)\n",
        "\n",
        "    # Make wget input file\n",
        "    url_list = []\n",
        "    for file in record_df['file'].tolist():\n",
        "        url_list.append('{}'.format(file))\n",
        "    with open(dataFolder + 'xc-noca-urls.txt', 'w+') as f:\n",
        "        for item in url_list:\n",
        "            f.write(\"{}\\n\".format(item))\n",
        "\n",
        "    # Get all soundfiles\n",
        "    os.system('wget -P ' + mp3Folder + ' --trust-server-names -i' + dataFolder + 'xc-noca-urls.txt')\n",
        "\n",
        "    files = os.listdir(mp3Folder)\n",
        "    [os.replace(mp3Folder + file, mp3Folder + file.replace(\" \", \"_\")) for file in files]\n",
        "\n",
        "\n",
        "def prepare_dataset(query, nbrOfTestSoundsForPrediction):\n",
        "    \"\"\"\n",
        "        Prepares the dataset for using in model\n",
        "        :param\n",
        "        query: Name of the bird. Blank space with %20\n",
        "        nbrOfTestSoundsForPrediction: Number of sounds copied to a\n",
        "        seperate folder for later predictions\n",
        "    \"\"\"\n",
        "    dataFolder = \"Data/\" + query.replace(\"%20\", \"_\") + \"/\"\n",
        "    mp3Folder = dataFolder + \"mp3/\"\n",
        "    arrayFolder = dataFolder + \"arrays/\"\n",
        "    predTestFolder = dataFolder + \"preTest/\"\n",
        "\n",
        "    os.mkdir(\"tmp\")\n",
        "\n",
        "    # The following line is only needed once if ffmpeg is not part of the PATH variables\n",
        "    # os.environ[\"PATH\"] += os.pathsep + r'F:\\ffmpeg\\bin'\n",
        "\n",
        "    # Reformat path string\n",
        "    globlist = glob.glob(mp3Folder + \"*.mp3\")\n",
        "    new_list = []\n",
        "    for string in globlist:\n",
        "        new_string = string.replace(\"\\\\\", \"/\")\n",
        "        new_list.append(new_string)\n",
        "\n",
        "    # Copy 5 entries to /predTest\n",
        "    last_elements = new_list[-nbrOfTestSoundsForPrediction:]\n",
        "    print(last_elements)\n",
        "\n",
        "    for file in last_elements:\n",
        "        shutil.copy(file, predTestFolder)\n",
        "        os.remove(file)\n",
        "\n",
        "    globlist.clear()\n",
        "    globlist = glob.glob(mp3Folder + \"*.mp3\")\n",
        "    new_list.clear()\n",
        "    for string in globlist:\n",
        "        new_string = string.replace(\"\\\\\", \"/\")\n",
        "        new_list.append(new_string)\n",
        "\n",
        "    # Extract frequencies and save them as np array\n",
        "    for file in new_list:\n",
        "        src = file\n",
        "        dst = \"tmp/tmp.wav\"\n",
        "\n",
        "        # convert mp3 to wav\n",
        "        sound = AudioSegment.from_mp3(src)\n",
        "        ten_seconds = 10 * 1000\n",
        "        first_10_seconds = sound[:ten_seconds]\n",
        "        first_10_seconds.export(dst, format=\"wav\")\n",
        "\n",
        "        y, sr = librosa.load(dst)\n",
        "\n",
        "        # CREATE A FIXED LENGTH\n",
        "        librosa.util.fix_length(y, 220500)\n",
        "\n",
        "        # EXTRACT FEATURES\n",
        "        mfccs_features = librosa.feature.mfcc(y, sr, n_mfcc=40)\n",
        "        mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
        "\n",
        "        # SAVE FEATURES TO FILE\n",
        "        index = new_list.index(file)\n",
        "        arrayPath = arrayFolder + str(index)\n",
        "        np.save(arrayPath, mfccs_scaled_features)\n",
        "\n",
        "        # Remove temp wav file\n",
        "        os.remove(dst)\n",
        "\n",
        "    # CREATE AN ARRAY OF ALL PICTURE FEATURES AND SAVE IT\n",
        "    arraylist = glob.glob(arrayFolder + \"*.npy\")\n",
        "    extracted_features = []\n",
        "    for file in arraylist:\n",
        "        data = np.load(file)\n",
        "        label = query.replace(\"%20\", \"_\")\n",
        "        extracted_features.append([data, label])\n",
        "\n",
        "    np.save(arrayFolder + \"summery_array\", extracted_features)\n",
        "\n",
        "\n",
        "def download_and_prepare_bird_dataset(query, nbrOfTestSoundsForPrediction):\n",
        "    \"\"\"\n",
        "        Downloads & prepares the dataset for using in model\n",
        "        :param\n",
        "        query: Name of the bird. Blank space with %20\n",
        "        nbrOfTestSoundsForPrediction: Number of sounds copied to a\n",
        "        seperate folder for later predictions\n",
        "    \"\"\"\n",
        "    downloas_bird_sounds(query)\n",
        "    prepare_dataset(query, nbrOfTestSoundsForPrediction)\n",
        "\n",
        "\n",
        "def show_spectogram_for_mp3(filepath):\n",
        "    \"\"\"\n",
        "        Shows the spectogram of a given mp3 path\n",
        "        :param\n",
        "        filepath: The filepath of the mp3\n",
        "    \"\"\"\n",
        "    src = filepath\n",
        "    dst = \"tmp/tmp.wav\"\n",
        "\n",
        "    # convert wav to mp3\n",
        "    sound = AudioSegment.from_mp3(src)\n",
        "    ten_seconds = 10 * 1000\n",
        "    first_10_seconds = sound[:ten_seconds]\n",
        "    first_10_seconds.export(dst, format=\"wav\")\n",
        "\n",
        "    y, sr = librosa.load(dst)\n",
        "    librosa.util.fix_length(y, 220500)\n",
        "\n",
        "    # SHOW WAVE\n",
        "    fig, ax = plt.subplots(nrows=3, sharex=True)\n",
        "    librosa.display.waveshow(y, sr=sr, ax=ax[0])\n",
        "    ax[0].set(title='Wave')\n",
        "\n",
        "    # SHOW SPEC\n",
        "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax[1])\n",
        "    ax[1].set(title='Linear-frequency power spectrogram')\n",
        "\n",
        "    # SHOW EXTRACTED FEATURES\n",
        "    mfccs_features = librosa.feature.mfcc(y, sr, n_mfcc=40)\n",
        "    img = librosa.display.specshow(mfccs_features, x_axis='time', ax=ax[2])\n",
        "    ax[2].set(title='mfccs_features')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perpare_mp3_for_prediction(filepath):\n",
        "    \"\"\"\n",
        "        Prepares the given mp3 for prediction\n",
        "        :param\n",
        "        filepath: The filepath of the mp3\n",
        "    \"\"\"\n",
        "    src = filepath\n",
        "    dst = \"tmp/tmp2.wav\"\n",
        "\n",
        "    # convert mp3 to wav\n",
        "    sound = AudioSegment.from_mp3(src)\n",
        "    ten_seconds = 10 * 1000\n",
        "    first_10_seconds = sound[:ten_seconds]\n",
        "    first_10_seconds.export(dst, format=\"wav\")\n",
        "\n",
        "    y, sr = librosa.load(dst)\n",
        "\n",
        "    # FIXED LENGTH\n",
        "    librosa.util.fix_length(y, 220500)\n",
        "\n",
        "    # EXTRACT FEATURES\n",
        "    mfccs_features = librosa.feature.mfcc(y, sr, n_mfcc=40)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
        "\n",
        "    # RESHAPE\n",
        "    mfccs_scaled_features = mfccs_scaled_features.reshape(1, -1)\n",
        "\n",
        "    # Remove temp wav file\n",
        "    os.remove(dst)\n",
        "\n",
        "    return mfccs_scaled_features\n",
        "\n",
        "\n",
        "def get_dataframe(query):\n",
        "    dataFolder = \"Data/\" + query.replace(\"%20\", \"_\") + \"/\"\n",
        "    arrayFolder = dataFolder + \"arrays/\"\n",
        "\n",
        "    numpy_data = np.load(arrayFolder + \"summery_array.npy\", allow_pickle=True)\n",
        "    extracted_features_df = pd.DataFrame(numpy_data, columns=['feature', 'label'])\n",
        "\n",
        "    return extracted_features_df\n",
        "\n",
        "\n",
        "def get_concat_dataframe(query_list):\n",
        "    result = pd.DataFrame()\n",
        "\n",
        "    for query in query_list:\n",
        "        df = get_dataframe(query)\n",
        "        result = result.append(df)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "Naec6t_-jeGI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "W0F1jwfikbEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
        "\n",
        "\n",
        "def create_and_fit_dense_model(batch_size, epochs, callbacks, X_train, X_test, y_train, y_test):\n",
        "    # BUILD THE MODEL\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_shape=(40,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(200))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    # TRAIN THE MODEL\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, callbacks=callbacks, epochs=epochs,\n",
        "                        validation_data=(X_test, y_test))\n",
        "\n",
        "    # SAVE MODEL AND HISTORY_DATA\n",
        "    np.save('Auswertung/history.npy', history.history)\n",
        "    model.save('Auswertung/model')\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def create_and_fit_cnn_model(batch_size, epochs, callbacks, X_train, X_test, y_train, y_test):\n",
        "    # BUILD THE MODEL\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, 3, activation='relu', input_shape=(40, 1)))\n",
        "    model.add(Conv1D(64, 3, activation='relu'))\n",
        "    model.add(MaxPooling1D(3))\n",
        "    model.add(Conv1D(128, 3, activation='relu'))\n",
        "    model.add(Conv1D(128, 3, activation='relu'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    # TRAIN THE MODEL\n",
        "    history_CNN = model.fit(X_train, y_train, batch_size=batch_size, callbacks=callbacks, epochs=epochs,\n",
        "                            validation_data=(X_test, y_test))\n",
        "\n",
        "    # SAVE MODEL AND HISTORY_DATA\n",
        "    np.save('Auswertung/history_CNN.npy', history_CNN.history)\n",
        "    model.save('Auswertung/model_CNN')\n",
        "\n",
        "    return history_CNN"
      ],
      "metadata": {
        "id": "26YA6nkskdI_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General helper functions"
      ],
      "metadata": {
        "id": "mHUKc00oksHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "    \"\"\"\n",
        "    Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "    (224, 224, 3).\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename (str): string filename of target image\n",
        "    img_shape (int): size to resize target image to, default 224\n",
        "    scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "    \"\"\"\n",
        "    # Read in the image\n",
        "    img = tf.io.read_file(filename)\n",
        "    # Decode it into a tensor\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    # Resize the image\n",
        "    img = tf.image.resize(img, [img_shape, img_shape])\n",
        "    if scale:\n",
        "        # Rescale the image (get all values between 0 and 1)\n",
        "        return img / 255.\n",
        "    else:\n",
        "        return img\n",
        "\n",
        "\n",
        "# Note: The following confusion matrix code is a remix of Scikit-Learn's\n",
        "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
        "    \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
        "    If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
        "    will be used.\n",
        "    Args:\n",
        "      y_true: Array of truth labels (must be same shape as y_pred).\n",
        "      y_pred: Array of predicted labels (must be same shape as y_true).\n",
        "      classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
        "      figsize: Size of output figure (default=(10, 10)).\n",
        "      text_size: Size of output figure text (default=15).\n",
        "      norm: normalize values or not (default=False).\n",
        "      savefig: save confusion matrix to file (default=False).\n",
        "    Returns:\n",
        "      A labelled confusion matrix plot comparing y_true and y_pred.\n",
        "    Example usage:\n",
        "      make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
        "                            y_pred=y_preds, # predicted labels\n",
        "                            classes=class_names, # array of class label names\n",
        "                            figsize=(15, 15),\n",
        "                            text_size=10)\n",
        "    \"\"\"\n",
        "    # Create the confustion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]  # normalize it\n",
        "    n_classes = cm.shape[0]  # find the number of classes we're dealing with\n",
        "\n",
        "    # Plot the figure and make it pretty\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    cax = ax.matshow(cm, cmap=plt.cm.Blues)  # colors will represent how 'correct' a class is, darker == better\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Are there a list of classes?\n",
        "    if classes:\n",
        "        labels = classes\n",
        "    else:\n",
        "        labels = np.arange(cm.shape[0])\n",
        "\n",
        "    # Label the axes\n",
        "    ax.set(title=\"Confusion Matrix\",\n",
        "           xlabel=\"Predicted label\",\n",
        "           ylabel=\"True label\",\n",
        "           xticks=np.arange(n_classes),  # create enough axis slots for each class\n",
        "           yticks=np.arange(n_classes),\n",
        "           xticklabels=labels,  # axes will labeled with class names (if they exist) or ints\n",
        "           yticklabels=labels)\n",
        "\n",
        "    # Make x-axis labels appear on bottom\n",
        "    ax.xaxis.set_label_position(\"bottom\")\n",
        "    ax.xaxis.tick_bottom()\n",
        "\n",
        "    # Set the threshold for different colors\n",
        "    threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "    # Plot the text on each cell\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if norm:\n",
        "            plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j] * 100:.1f}%)\",\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "                     size=text_size)\n",
        "        else:\n",
        "            plt.text(j, i, f\"{cm[i, j]}\",\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "                     size=text_size)\n",
        "\n",
        "    # Save the figure to the current working directory\n",
        "    if savefig:\n",
        "        fig.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "\n",
        "# Make a function to predict on images and plot them (works with multi-class)\n",
        "def pred_and_plot(model, filename, class_names, scale=True, shape=224):\n",
        "    \"\"\"\n",
        "    Imports an image located at filename, makes a prediction on it with\n",
        "    a trained model and plots the image with the predicted class as the title.\n",
        "    \"\"\"\n",
        "    # Import the target image and preprocess it\n",
        "    img = load_and_prep_image(filename, shape, scale)\n",
        "\n",
        "    # Make a prediction\n",
        "    pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "    # Get the predicted class\n",
        "    if len(pred[0]) > 1:  # check for multi-class\n",
        "        pred_class = class_names[pred.argmax()]  # if more than one output, take the max\n",
        "    else:\n",
        "        pred_class = class_names[int(tf.round(pred)[0][0])]  # if only one output, round\n",
        "\n",
        "    # Plot the image and predicted class\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Prediction: {pred_class}\")\n",
        "    plt.axis(False);\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "    \"\"\"\n",
        "    Creates a TensorBoard callback instand to store log files.\n",
        "    Stores log files with the filepath:\n",
        "      \"dir_name/experiment_name/current_datetime/\"\n",
        "    Args:\n",
        "      dir_name: target directory to store TensorBoard log files\n",
        "      experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "    \"\"\"\n",
        "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=log_dir\n",
        "    )\n",
        "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "    return tensorboard_callback\n",
        "\n",
        "\n",
        "# Plot the validation and training data separately\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_loss_curves(history):\n",
        "    \"\"\"\n",
        "    Returns separate loss curves for training and validation metrics.\n",
        "    Args:\n",
        "      history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
        "    \"\"\"\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    epochs = range(len(history.history['loss']))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.plot(epochs, loss, label='training_loss')\n",
        "    plt.plot(epochs, val_loss, label='val_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def launchTensorBoard(log_dir):\n",
        "    import os\n",
        "    os.system('tensorboard --logdir=' + log_dir)\n",
        "    return\n",
        "\n",
        "\n",
        "def compare_historys(original_history, new_history, initial_epochs=5):\n",
        "    \"\"\"\n",
        "    Compares two TensorFlow model History objects.\n",
        "    Args:\n",
        "      original_history: History object from original model (before new_history)\n",
        "      new_history: History object from continued model training (after original_history)\n",
        "      initial_epochs: Number of epochs in original_history (new_history plot starts from here)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get original history measurements\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "\n",
        "    # Combine original history with new history\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    # Make plots\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.plot([initial_epochs - 1, initial_epochs - 1],\n",
        "             plt.ylim(), label='Start Fine Tuning')  # reshift plot around epochs\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.plot([initial_epochs - 1, initial_epochs - 1],\n",
        "             plt.ylim(), label='Start Fine Tuning')  # reshift plot around epochs\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Create function to unzip a zipfile into current working directory\n",
        "# (since we're going to be downloading and unzipping a few files)\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def unzip_data(filename):\n",
        "    \"\"\"\n",
        "    Unzips filename into the current working directory.\n",
        "    Args:\n",
        "      filename (str): a filepath to a target zip folder to be unzipped.\n",
        "    \"\"\"\n",
        "    zip_ref = zipfile.ZipFile(filename, \"r\")\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()\n",
        "\n",
        "\n",
        "# Walk through an image classification directory and find out how many files (images)\n",
        "# are in each subdirectory.\n",
        "import os\n",
        "\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "    \"\"\"\n",
        "    Walks through dir_path returning its contents.\n",
        "    Args:\n",
        "      dir_path (str): target directory\n",
        "    Returns:\n",
        "      A print out of:\n",
        "        number of subdiretories in dir_path\n",
        "        number of images (files) in each subdirectory\n",
        "        name of each subdirectory\n",
        "    \"\"\"\n",
        "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "\n",
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "    Args:\n",
        "        y_true: true labels in the form of a 1D array\n",
        "        y_pred: predicted labels in the form of a 1D array\n",
        "    Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "    \"\"\"\n",
        "    # Calculate model accuracy\n",
        "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    # Calculate model precision, recall and f1 score using \"weighted average\n",
        "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "    model_results = {\"accuracy\": model_accuracy,\n",
        "                     \"precision\": model_precision,\n",
        "                     \"recall\": model_recall,\n",
        "                     \"f1\": model_f1}\n",
        "    return model_results\n",
        "\n",
        "\n",
        "# View an image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "\n",
        "def view_random_image(target_dir, target_class):\n",
        "    # Setup target directory (we'll view images from here)\n",
        "    target_folder = target_dir + target_class\n",
        "\n",
        "    # Get a random image path\n",
        "    random_image = random.sample(os.listdir(target_folder), 1)\n",
        "\n",
        "    # Read in the image and plot it using matplotlib\n",
        "    img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
        "    plt.imshow(img)\n",
        "    plt.title(target_class)\n",
        "    plt.axis(\"off\");\n",
        "    plt.show();\n",
        "\n",
        "    print(f\"Image shape: {img.shape}\")  # show the shape of the image\n",
        "\n",
        "    return img\n",
        "\n",
        "def plot_saved_loss_curves(filename):\n",
        "    \"\"\"\n",
        "    Returns separate loss curves for training and validation metrics.\n",
        "    Args:\n",
        "      history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
        "    \"\"\"\n",
        "    history = np.load(filename, allow_pickle='TRUE').item()\n",
        "\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "\n",
        "    accuracy = history['sparse_categorical_accuracy']\n",
        "    val_accuracy = history['val_sparse_categorical_accuracy']\n",
        "\n",
        "    epochs = range(len(history['loss']))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.plot(epochs, loss, label='training_loss')\n",
        "    plt.plot(epochs, val_loss, label='val_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "62lkipEdkvHm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "emTJ_SVeipH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOWNLOAD DATASETS\n",
        "IS ONLY NEEDED IF THERE'RE NO DATA PREVIOUSLY DOWNLOADED"
      ],
      "metadata": {
        "id": "T5tXz51HjJPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_prepare_bird_dataset('northern%20cardinal', 5)\n",
        "download_and_prepare_bird_dataset('Gaviidae', 5)\n",
        "download_and_prepare_bird_dataset('Crypturellus%20cinereus', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "iNe_gucYjCdR",
        "outputId": "78c3f88e-5e06-483c-f55f-4098eded5d4b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ffda0612cef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_and_prepare_bird_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'northern%20cardinal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdownload_and_prepare_bird_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gaviidae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdownload_and_prepare_bird_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Crypturellus%20cinereus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-40e384df1867>\u001b[0m in \u001b[0;36mdownload_and_prepare_bird_dataset\u001b[0;34m(query, nbrOfTestSoundsForPrediction)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mseperate\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlater\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \"\"\"\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mdownloas_bird_sounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrOfTestSoundsForPrediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-40e384df1867>\u001b[0m in \u001b[0;36mdownloas_bird_sounds\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpredTestFolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"preTest/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp3Folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'Data'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Show Spectograms"
      ],
      "metadata": {
        "id": "SaYkFQ9VmPPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_spectogram_for_mp3('Data/Gaviidae/preTest/XC681331-Great_Northern_Diver_Skaw_Whalsay_SG_Brambling.mp3')"
      ],
      "metadata": {
        "id": "XUrXi1eOmUeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare data and callbacks"
      ],
      "metadata": {
        "id": "n7n2q7nBl43b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GET DATAFRAMES\n",
        "train_df = get_concat_dataframe(['northern%20cardinal', 'Gaviidae', 'Crypturellus%20cinereus'])\n",
        "\n",
        "data = np.array(train_df['feature'].tolist())\n",
        "labels = np.array(train_df['label'].tolist())\n",
        "\n",
        "# ENCODE LABELS\n",
        "labels_encoded = pd.get_dummies(labels)\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)\n",
        "np.save('Auswertung/classes.npy', encoder.classes_)\n",
        "\n",
        "# SPLIT TRAINING AND VALIDATION DATA\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, shuffle=True, test_size=0.3, random_state=42)\n",
        "\n",
        "# CALLBACKS\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', min_lr=0.001, patience=10, mode='min', verbose=1)\n",
        "early_stopping = EarlyStopping(patience=15, monitor='val_accuracy')\n",
        "tensorboard = create_tensorboard_callback('Auswertung/', 'BirdSoundPrediction')\n",
        "callbacks = [tensorboard]"
      ],
      "metadata": {
        "id": "PIjktH2hl9X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create and fit model"
      ],
      "metadata": {
        "id": "Y_fK3ZEfluvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#history = create_and_fit_dense_model(32, 148, callbacks, X_train, X_test, y_train, y_test)\n",
        "history = create_and_fit_cnn_model(32, 15, callbacks, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# PLOT THE TRAINING CURVES\n",
        "plot_loss_curves(history)"
      ],
      "metadata": {
        "id": "lfKJ6d8-lxia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predictions"
      ],
      "metadata": {
        "id": "zp1WLdpClTb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Load model\n",
        "#model = keras.models.load_model('Auswertung/model')\n",
        "model_CNN = keras.models.load_model('Auswertung/model_CNN')\n",
        "\n",
        "#data = perpare_mp3_for_prediction('Data/Gaviidae/preTest/XC680885-GAVSTE2021-07-14-0644-TOLK-c-m.mp3')\n",
        "#data = perpare_mp3_for_prediction('Data/northern_cardinal/mp3/CardinalKeyWest.mp3')\n",
        "#data = perpare_mp3_for_prediction('Data/Crypturellus_cinereus/mp3/Cinereous_tinamou1.mp3')\n",
        "\n",
        "# Make a prediction\n",
        "predArray = model_CNN.predict(data)\n",
        "pred = np.argmax(predArray, axis=1)\n",
        "\n",
        "print(pred)\n",
        "\n",
        "classes = np.load('Auswertung/classes.npy', allow_pickle=True)\n",
        "print(classes[pred])"
      ],
      "metadata": {
        "id": "yYPPMrTJlWqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Show in Tensorboard"
      ],
      "metadata": {
        "id": "ccJCXMK1lAj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "t = threading.Thread(target=launchTensorBoard('Auswertung/BirdSoundPrediction/20211223-093239'), args=([]))\n",
        "t.start()"
      ],
      "metadata": {
        "id": "35NogAtdlDDK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}